{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from pytz import timezone\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix the random seed to reproducibility\n",
    "# np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_CU():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_SP500():\n",
    "    import pandas_datareader as pdr    \n",
    "    SP500 = pdr.get_data_yahoo('^GSPC')\n",
    "    return SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_data():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_X_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high = X.high.values\n",
    "low = X.low.values\n",
    "close = X.close.values\n",
    "volume = X.volume.astype(np.float64).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Max Scaler\n",
    "X[\"%K\"], X[\"%D\"] = talib.STOCHF(X.high.values, X.low.values, X.close.values)\n",
    "_, X[\"Slow %D\"] = talib.STOCH(X.high.values, X.low.values, X.close.values)\n",
    "X[\"RSI\"] = talib.RSI(close)\n",
    "X[\"WilliamsR\"] = talib.WILLR(X.high.values, X.low.values, X.close.values)\n",
    "X[\"-DI\"] = talib.MINUS_DI(high, low, close)\n",
    "X[\"+DI\"] = talib.PLUS_DI(high, low, close)\n",
    "X[\"CMO\"] = talib.CMO(close)\n",
    "X[\"AroonOSC\"] = talib.AROONOSC(high, low)\n",
    "X[\"ADX\"] = talib.ADX(high, low, close)\n",
    "X[\"AroonDown\"], X[\"AroonUp\"] = talib.AROON(high, low)\n",
    "X[\"ADXR\"] = talib.ADXR(high, low, close)\n",
    "\n",
    "_top, _mid, _bot = talib.BBANDS(close)\n",
    "diff = _top - _bot\n",
    "diff[(_top - _bot) < 1] = 1\n",
    "X[\"%B\"] = (X.close - _bot)/diff\n",
    "X[\"UltimateOscillator\"] = talib.ULTOSC(high, low, close)\n",
    "X[\"MFI\"] = talib.MFI(high, low, close, volume)\n",
    "\n",
    "# Scale the same as raw prices\n",
    "\"\"\"\n",
    "X[\"DEMA\"] = talib.DEMA(close)\n",
    "X[\"EMA\"] = talib.EMA(close)\n",
    "X[\"kAMA\"] = talib.KAMA(close)\n",
    "X[\"TEMA\"] = talib.TEMA(close)\n",
    "X[\"TRIMA\"] = talib.TRIMA(close)\n",
    "X[\"WMA\"] = talib.WMA(close)\n",
    "X[\"MA\"] = talib.MA(close)\n",
    "X[\"MAMA\"], _ = talib.MAMA(close)\n",
    "X[\"BBTop\"], X[\"BBMid\"], X[\"BBBot\"] = talib.BBANDS(close)\n",
    "X[\"HHV\"] = talib.MAX(close)\n",
    "X[\"LLV\"] = talib.MIN(close)\n",
    "X[\"ParabolicSAR\"] = talib.SAR(high, low)\n",
    "\n",
    "# No Scaling Needed\n",
    "X[\"BOP\"] = talib.BOP(X.open.values, high, low, close)\n",
    "X[\"PVI\"] = (close - X.open.values) / (high - low)\n",
    "\n",
    "# Scale as percentage\n",
    "X[\"ROC\"] = talib.ROC(X.close.values)\n",
    "\n",
    "# No Sure what to do\n",
    "X[\"Momentum\"] = talib.MOM(X.close.values)\n",
    "X[\"CCI\"] = talib.CCI(high, low, close)\n",
    "X[\"APO\"] = talib.APO(close)\n",
    "X[\"-DM\"] = talib.MINUS_DM(high, low)\n",
    "X[\"+DM\"] = talib.PLUS_DM(high, low)\n",
    "X[\"ATR\"] = talib.ATR(high, low, close)\n",
    "X[\"BBWidth\"] = X[\"BBTop\"] - X[\"BBBot\"]\n",
    "X[\"MACD\"], X[\"MACDSig\"], X[\"MACDHist\"] = talib.MACD(close)\n",
    "X[\"PPO\"] = talib.PPO(close)\n",
    "X[\"ADOscillator\"] = talib.ADOSC(high, low, close, volume)\n",
    "X[\"TRIX\"] = talib.TRIX(close)\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 33.35%\n",
      "Train: 66.65%\n"
     ]
    }
   ],
   "source": [
    "X_train = X[\"2012\":\"2015\"]\n",
    "X_test = X[\"2016\":]\n",
    "print(\"Test: {:.2f}%\".format(100 * len(X_test)/len(X[\"2012\":])))\n",
    "print(\"Train: {:.2f}%\".format(100 * len(X_train)/len(X[\"2012\":])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166845"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333465"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958635, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not (X[\"2012\":] == np.inf).any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    \"%K\": MinMaxScaler(),\n",
    "    \"%D\": MinMaxScaler(),\n",
    "    \"Slow %D\": MinMaxScaler(),\n",
    "    \"RSI\": MinMaxScaler(),\n",
    "    \"WilliamsR\": MinMaxScaler(),\n",
    "    \"-DI\": MinMaxScaler(),\n",
    "    \"+DI\": MinMaxScaler(),\n",
    "    \"CMO\": MinMaxScaler(),\n",
    "    \"AroonOSC\": MinMaxScaler(),\n",
    "    \"ADX\": MinMaxScaler(),\n",
    "    \"AroonDown\": MinMaxScaler(),\n",
    "    \"AroonUp\": MinMaxScaler(),\n",
    "    \"ADXR\": MinMaxScaler(),\n",
    "    \"UltimateOscillator\": MinMaxScaler(),\n",
    "    \"MFI\": MinMaxScaler(),\n",
    "    \"%B\": MinMaxScaler()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train['open']\n",
    "del X_train['high'] \n",
    "del X_train['low']\n",
    "del X_train['close']\n",
    "del X_train['volume']\n",
    "del X_train['openint']\n",
    "\n",
    "del X_test['open']\n",
    "del X_test['high'] \n",
    "del X_test['low']\n",
    "del X_test['close']\n",
    "del X_test['volume']\n",
    "del X_test['openint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_simple_autoencoder():\n",
    "    encoding_dim = 8 # 54 dim -> 16 dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(X_train.shape[1])(encoded)\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    \n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss='mse')\n",
    "    history = autoencoder.fit(X_train.values, X_train.values, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)\n",
    "    \n",
    "    return (history, autoencoder, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0158 - val_loss: 0.0044\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0041 - val_loss: 0.0034\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0022 - val_loss: 0.0016\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 77/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 12s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "SAE_results = train_simple_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_autoencoder(dim=8):\n",
    "    encoding_dim = 8 # 54 dim -> 16 dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    encoded = Dense(int(X_train.shape[1]), activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    decoded = Dense(encoding_dim*2, activation='sigmoid')(encoded)    \n",
    "    decoded = Dense(X_train.shape[1])(decoded)\n",
    "    \n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    decoder_layer2 = autoencoder.layers[-2]\n",
    "    decoder_layer1 = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer1(decoder_layer2(encoded_input)))\n",
    "    \n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss='mse')\n",
    "    history = autoencoder.fit(X_train.values, X_train.values, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)\n",
    "    \n",
    "    return (history, autoencoder, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_deep3_autoencoder(dim=8):\n",
    "    encoding_dim = 8 # 54 dim -> 16 dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    encoded = Dense(14, activation='relu')(input_layer)\n",
    "    encoded = Dense(12, activation='relu')(encoded)\n",
    "    encoded = Dense(10, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    decoded = Dense(10, activation='relu')(encoded)\n",
    "    decoded = Dense(12, activation='relu')(decoded)\n",
    "    decoded = Dense(14, activation='relu')(decoded)\n",
    "    decoded = Dense(X_train.shape[1])(decoded)\n",
    "    \n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    decoder_layer4 = autoencoder.layers[-4]\n",
    "    decoder_layer3 = autoencoder.layers[-3]\n",
    "    decoder_layer2 = autoencoder.layers[-2]\n",
    "    decoder_layer1 = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer1(decoder_layer2(decoder_layer3(decoder_layer4(encoded_input)))))\n",
    "    \n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss='mse')\n",
    "    history = autoencoder.fit(X_train.values, X_train.values, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)\n",
    "    \n",
    "    return (history, autoencoder, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0179 - val_loss: 0.0044\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0011 - val_loss: 9.8988e-04\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0011 - val_loss: 9.8188e-04\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.7693e-04\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.7352e-04\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.7102e-04\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6911e-04\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6760e-04\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6635e-04\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6527e-04\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6431e-04\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6344e-04\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6261e-04\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6182e-04\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.6102e-04\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.6037e-04\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5969e-04\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5897e-04\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5823e-04\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5743e-04\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5656e-04\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5560e-04\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5454e-04\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5338e-04\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5207e-04\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5062e-04\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.4899e-04\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.4720e-04\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.4523e-04\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9865e-04 - val_loss: 9.4309e-04\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9650e-04 - val_loss: 9.4081e-04\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9430e-04 - val_loss: 9.3839e-04\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9207e-04 - val_loss: 9.3588e-04\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8982e-04 - val_loss: 9.3330e-04\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8758e-04 - val_loss: 9.3071e-04\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8538e-04 - val_loss: 9.2814e-04\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8323e-04 - val_loss: 9.2562e-04\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8114e-04 - val_loss: 9.2318e-04\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7912e-04 - val_loss: 9.2085e-04\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7719e-04 - val_loss: 9.1863e-04\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7534e-04 - val_loss: 9.1654e-04\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7357e-04 - val_loss: 9.1458e-04\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7190e-04 - val_loss: 9.1276e-04\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7031e-04 - val_loss: 9.1105e-04\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.6880e-04 - val_loss: 9.0946e-04\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.6738e-04 - val_loss: 9.0798e-04\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.6602e-04 - val_loss: 9.0659e-04\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6474e-04 - val_loss: 9.0529e-04\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6352e-04 - val_loss: 9.0407e-04\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6235e-04 - val_loss: 9.0291e-04\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6125e-04 - val_loss: 9.0181e-04\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6018e-04 - val_loss: 9.0078e-04\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.5916e-04 - val_loss: 8.9979e-04\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5818e-04 - val_loss: 8.9884e-04\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.5724e-04 - val_loss: 8.9792e-04\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5631e-04 - val_loss: 8.9704e-04\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5542e-04 - val_loss: 8.9620e-04\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5455e-04 - val_loss: 8.9537e-04\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5369e-04 - val_loss: 8.9455e-04\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5284e-04 - val_loss: 8.9374e-04\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5201e-04 - val_loss: 8.9293e-04\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5118e-04 - val_loss: 8.9212e-04\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5035e-04 - val_loss: 8.9131e-04\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.4952e-04 - val_loss: 8.9048e-04\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.4869e-04 - val_loss: 8.8966e-04\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.4786e-04 - val_loss: 8.8882e-04\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4703e-04 - val_loss: 8.8797e-04\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4620e-04 - val_loss: 8.8711e-04\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4537e-04 - val_loss: 8.8626e-04\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4455e-04 - val_loss: 8.8540e-04\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4373e-04 - val_loss: 8.8455e-04\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 15s - loss: 9.4292e-04 - val_loss: 8.8368e-04\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4212e-04 - val_loss: 8.8284e-04\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4132e-04 - val_loss: 8.8199e-04\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4053e-04 - val_loss: 8.8116e-04\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3976e-04 - val_loss: 8.8033e-04\n",
      "Epoch 82/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3899e-04 - val_loss: 8.7951e-04\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3824e-04 - val_loss: 8.7870e-04\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3751e-04 - val_loss: 8.7791e-04\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3679e-04 - val_loss: 8.7713e-04\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3609e-04 - val_loss: 8.7636e-04\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3540e-04 - val_loss: 8.7560e-04\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3472e-04 - val_loss: 8.7485e-04\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3406e-04 - val_loss: 8.7412e-04\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3342e-04 - val_loss: 8.7340e-04\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3278e-04 - val_loss: 8.7269e-04\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3216e-04 - val_loss: 8.7199e-04\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3154e-04 - val_loss: 8.7130e-04\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3092e-04 - val_loss: 8.7062e-04\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3030e-04 - val_loss: 8.6994e-04\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2968e-04 - val_loss: 8.6926e-04\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2906e-04 - val_loss: 8.6859e-04\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2843e-04 - val_loss: 8.6791e-04\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2780e-04 - val_loss: 8.6724e-04\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2715e-04 - val_loss: 8.6656e-04\n"
     ]
    }
   ],
   "source": [
    "deep_results = train_deep_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=1e-4, decay=1e-9, momentum=0.9, nesterov=True, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0142 - val_loss: 0.0063\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0054 - val_loss: 0.0049\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0047 - val_loss: 0.0038\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0024 - val_loss: 0.0021\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 27s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 27s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 27s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 77/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 25s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 24s - loss: 0.0023 - val_loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "deep3_results = train_deep3_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0155 - val_loss: 0.0041\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 77/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96400511,  0.8872205 ,  0.8110007 , ...,  0.95634049,\n",
       "         0.6861577 ,  0.96675026],\n",
       "       [ 0.92161393,  0.87873733,  0.83197844, ...,  0.91126007,\n",
       "         0.69899333,  1.00022972],\n",
       "       [ 0.82803655,  0.93806481,  0.9290818 , ...,  0.77344495,\n",
       "         0.69129795,  0.92499137],\n",
       "       ..., \n",
       "       [ 0.17240553,  0.19646129,  0.219879  , ...,  0.25193104,\n",
       "         0.35655159,  0.28895956],\n",
       "       [ 0.54303604,  0.31664708,  0.19415942, ...,  0.64015359,\n",
       "         0.38101375,  0.33227432],\n",
       "       [ 0.95092154,  0.52763247,  0.34683344, ...,  0.99865854,\n",
       "         0.51167536,  0.4727087 ]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.predict(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x136c3b588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "size_limit = 40\n",
    "# plt.plot(SAE_results[0].history['loss'][:size_limit])\n",
    "plt.plot(SAE_results[0].history['val_loss'][:size_limit])\n",
    "# plt.plot(deep_results[0].history['loss'][:size_limit])\n",
    "plt.plot(deep_results[0].history['val_loss'][:size_limit])\n",
    "plt.plot(denoising_results[0].history['val_loss'][:size_limit])\n",
    "plt.plot(deep3_results[0].history['val_loss'][:size_limit])\n",
    "\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Simple AE (2 Layer) test', \"AE (4 Layer) test\", \"Denoising AE (4 Layer)\", \"Deep AE (8 layers)\", ], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_Denoising_AutoEncoder(dim=8):\n",
    "    encoding_dim = 8 # 54 dim -> 16 dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    encoded = Dense(int(X_train.shape[1]), activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    decoded = Dense(encoding_dim*2, activation='sigmoid')(encoded)    \n",
    "    decoded = Dense(X_train.shape[1])(decoded)\n",
    "    \n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    decoder_layer2 = autoencoder.layers[-2]\n",
    "    decoder_layer1 = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer1(decoder_layer2(encoded_input)))\n",
    "    \n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss='mse')\n",
    "    \n",
    "    noise_factor = 0.1\n",
    "    x_train_noisy = X_train.values + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "    x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "    \n",
    "    history = autoencoder.fit(x_train_noisy, X_train.values, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)\n",
    "    \n",
    "    return (history, autoencoder, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0181 - val_loss: 0.0071\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0072 - val_loss: 0.0065\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 77/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 17s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0045 - val_loss: 0.0044\n"
     ]
    }
   ],
   "source": [
    "denoising_results = train_Denoising_AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DeepAE = deep_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165408/166845 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0009308088173139293"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DeepAE.evaluate(X_test.values, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAE = denoising_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165504/166845 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.010075671662965531"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAE.evaluate(X_test.values, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAE = SAE_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166144/166845 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0014700626765022863"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAE.evaluate(X_test.values, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
