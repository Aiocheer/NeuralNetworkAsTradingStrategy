{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from pytz import timezone\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix the random seed to reproducibility\n",
    "# np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_CU():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_SP500():\n",
    "    import pandas_datareader as pdr    \n",
    "    SP500 = pdr.get_data_yahoo('^GSPC')\n",
    "    return SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_data():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_X_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high = X.high.values\n",
    "low = X.low.values\n",
    "close = X.close.values\n",
    "volume = X.volume.astype(np.float64).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Max Scaler\n",
    "X[\"%K\"], X[\"%D\"] = talib.STOCHF(X.high.values, X.low.values, X.close.values)\n",
    "_, X[\"Slow %D\"] = talib.STOCH(X.high.values, X.low.values, X.close.values)\n",
    "X[\"RSI\"] = talib.RSI(close)\n",
    "X[\"WilliamsR\"] = talib.WILLR(X.high.values, X.low.values, X.close.values)\n",
    "X[\"-DI\"] = talib.MINUS_DI(high, low, close)\n",
    "X[\"+DI\"] = talib.PLUS_DI(high, low, close)\n",
    "X[\"CMO\"] = talib.CMO(close)\n",
    "X[\"AroonOSC\"] = talib.AROONOSC(high, low)\n",
    "X[\"ADX\"] = talib.ADX(high, low, close)\n",
    "X[\"AroonDown\"], X[\"AroonUp\"] = talib.AROON(high, low)\n",
    "X[\"ADXR\"] = talib.ADXR(high, low, close)\n",
    "\n",
    "_top, _mid, _bot = talib.BBANDS(close)\n",
    "diff = _top - _bot\n",
    "diff[(_top - _bot) < 1] = 1\n",
    "X[\"%B\"] = (X.close - _bot)/diff\n",
    "X[\"UltimateOscillator\"] = talib.ULTOSC(high, low, close)\n",
    "X[\"MFI\"] = talib.MFI(high, low, close, volume)\n",
    "\n",
    "# Scale the same as raw prices\n",
    "\"\"\"\n",
    "X[\"DEMA\"] = talib.DEMA(close)\n",
    "X[\"EMA\"] = talib.EMA(close)\n",
    "X[\"kAMA\"] = talib.KAMA(close)\n",
    "X[\"TEMA\"] = talib.TEMA(close)\n",
    "X[\"TRIMA\"] = talib.TRIMA(close)\n",
    "X[\"WMA\"] = talib.WMA(close)\n",
    "X[\"MA\"] = talib.MA(close)\n",
    "X[\"MAMA\"], _ = talib.MAMA(close)\n",
    "X[\"BBTop\"], X[\"BBMid\"], X[\"BBBot\"] = talib.BBANDS(close)\n",
    "X[\"HHV\"] = talib.MAX(close)\n",
    "X[\"LLV\"] = talib.MIN(close)\n",
    "X[\"ParabolicSAR\"] = talib.SAR(high, low)\n",
    "\n",
    "# No Scaling Needed\n",
    "X[\"BOP\"] = talib.BOP(X.open.values, high, low, close)\n",
    "X[\"PVI\"] = (close - X.open.values) / (high - low)\n",
    "\n",
    "# Scale as percentage\n",
    "X[\"ROC\"] = talib.ROC(X.close.values)\n",
    "\n",
    "# No Sure what to do\n",
    "X[\"Momentum\"] = talib.MOM(X.close.values)\n",
    "X[\"CCI\"] = talib.CCI(high, low, close)\n",
    "X[\"APO\"] = talib.APO(close)\n",
    "X[\"-DM\"] = talib.MINUS_DM(high, low)\n",
    "X[\"+DM\"] = talib.PLUS_DM(high, low)\n",
    "X[\"ATR\"] = talib.ATR(high, low, close)\n",
    "X[\"BBWidth\"] = X[\"BBTop\"] - X[\"BBBot\"]\n",
    "X[\"MACD\"], X[\"MACDSig\"], X[\"MACDHist\"] = talib.MACD(close)\n",
    "X[\"PPO\"] = talib.PPO(close)\n",
    "X[\"ADOscillator\"] = talib.ADOSC(high, low, close, volume)\n",
    "X[\"TRIX\"] = talib.TRIX(close)\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 33.35%\n",
      "Train: 66.65%\n"
     ]
    }
   ],
   "source": [
    "X_train = X[\"2012\":\"2015\"]\n",
    "X_test = X[\"2016\":]\n",
    "print(\"Test: {:.2f}%\".format(100 * len(X_test)/len(X[\"2012\":])))\n",
    "print(\"Train: {:.2f}%\".format(100 * len(X_train)/len(X[\"2012\":])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166845"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333465"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958635, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not (X[\"2012\":] == np.inf).any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    \"%K\": MinMaxScaler(),\n",
    "    \"%D\": MinMaxScaler(),\n",
    "    \"Slow %D\": MinMaxScaler(),\n",
    "    \"RSI\": MinMaxScaler(),\n",
    "    \"WilliamsR\": MinMaxScaler(),\n",
    "    \"-DI\": MinMaxScaler(),\n",
    "    \"+DI\": MinMaxScaler(),\n",
    "    \"CMO\": MinMaxScaler(),\n",
    "    \"AroonOSC\": MinMaxScaler(),\n",
    "    \"ADX\": MinMaxScaler(),\n",
    "    \"AroonDown\": MinMaxScaler(),\n",
    "    \"AroonUp\": MinMaxScaler(),\n",
    "    \"ADXR\": MinMaxScaler(),\n",
    "    \"UltimateOscillator\": MinMaxScaler(),\n",
    "    \"MFI\": MinMaxScaler(),\n",
    "    \"%B\": MinMaxScaler()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train['open']\n",
    "del X_train['high'] \n",
    "del X_train['low']\n",
    "del X_train['close']\n",
    "del X_train['volume']\n",
    "del X_train['openint']\n",
    "\n",
    "del X_test['open']\n",
    "del X_test['high'] \n",
    "del X_test['low']\n",
    "del X_test['close']\n",
    "del X_test['volume']\n",
    "del X_test['openint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_simple_autoencoder(encoder):\n",
    "    encoding_dim = 8 # 54 dim -> 16 dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(X_train.shape[1])(encoded)\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    \n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss='mse')\n",
    "    history = autoencoder.fit(X_train.values, X_train.values, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)\n",
    "    \n",
    "    return (history, autoencoder, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_autoencoder(dim=8):\n",
    "    encoding_dim = 8 # 54 dim -> 16 dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    \n",
    "    encoded = Dense(int(X_train.shape[1]), activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    decoded = Dense(encoding_dim*2, activation='sigmoid')(encoded)    \n",
    "    decoded = Dense(X_train.shape[1])(decoded)\n",
    "    \n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    decoder_layer2 = autoencoder.layers[-2]\n",
    "    decoder_layer1 = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer1(decoder_layer2(encoded_input)))\n",
    "    \n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss='mse')\n",
    "    history = autoencoder.fit(X_train.values, X_train.values, epochs=100, batch_size=32, validation_split=0.2, shuffle=False)\n",
    "    \n",
    "    return (history, autoencoder, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0179 - val_loss: 0.0044\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0011 - val_loss: 9.8988e-04\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0011 - val_loss: 9.8188e-04\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.7693e-04\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.7352e-04\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.7102e-04\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6911e-04\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6760e-04\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6635e-04\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6527e-04\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6431e-04\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6344e-04\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6261e-04\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.6182e-04\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.6102e-04\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.6037e-04\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5969e-04\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5897e-04\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5823e-04\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5743e-04\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5656e-04\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5560e-04\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5454e-04\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5338e-04\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 15s - loss: 0.0010 - val_loss: 9.5207e-04\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.5062e-04\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.4899e-04\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.4720e-04\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 16s - loss: 0.0010 - val_loss: 9.4523e-04\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9865e-04 - val_loss: 9.4309e-04\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9650e-04 - val_loss: 9.4081e-04\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9430e-04 - val_loss: 9.3839e-04\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.9207e-04 - val_loss: 9.3588e-04\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8982e-04 - val_loss: 9.3330e-04\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8758e-04 - val_loss: 9.3071e-04\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8538e-04 - val_loss: 9.2814e-04\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8323e-04 - val_loss: 9.2562e-04\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.8114e-04 - val_loss: 9.2318e-04\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7912e-04 - val_loss: 9.2085e-04\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7719e-04 - val_loss: 9.1863e-04\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7534e-04 - val_loss: 9.1654e-04\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7357e-04 - val_loss: 9.1458e-04\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7190e-04 - val_loss: 9.1276e-04\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.7031e-04 - val_loss: 9.1105e-04\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.6880e-04 - val_loss: 9.0946e-04\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.6738e-04 - val_loss: 9.0798e-04\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.6602e-04 - val_loss: 9.0659e-04\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6474e-04 - val_loss: 9.0529e-04\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6352e-04 - val_loss: 9.0407e-04\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6235e-04 - val_loss: 9.0291e-04\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6125e-04 - val_loss: 9.0181e-04\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.6018e-04 - val_loss: 9.0078e-04\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.5916e-04 - val_loss: 8.9979e-04\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5818e-04 - val_loss: 8.9884e-04\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.5724e-04 - val_loss: 8.9792e-04\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5631e-04 - val_loss: 8.9704e-04\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5542e-04 - val_loss: 8.9620e-04\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5455e-04 - val_loss: 8.9537e-04\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5369e-04 - val_loss: 8.9455e-04\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5284e-04 - val_loss: 8.9374e-04\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5201e-04 - val_loss: 8.9293e-04\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5118e-04 - val_loss: 8.9212e-04\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.5035e-04 - val_loss: 8.9131e-04\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.4952e-04 - val_loss: 8.9048e-04\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.4869e-04 - val_loss: 8.8966e-04\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.4786e-04 - val_loss: 8.8882e-04\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4703e-04 - val_loss: 8.8797e-04\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4620e-04 - val_loss: 8.8711e-04\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4537e-04 - val_loss: 8.8626e-04\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4455e-04 - val_loss: 8.8540e-04\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4373e-04 - val_loss: 8.8455e-04\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 15s - loss: 9.4292e-04 - val_loss: 8.8368e-04\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4212e-04 - val_loss: 8.8284e-04\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4132e-04 - val_loss: 8.8199e-04\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.4053e-04 - val_loss: 8.8116e-04\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3976e-04 - val_loss: 8.8033e-04\n",
      "Epoch 82/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3899e-04 - val_loss: 8.7951e-04\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3824e-04 - val_loss: 8.7870e-04\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3751e-04 - val_loss: 8.7791e-04\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3679e-04 - val_loss: 8.7713e-04\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3609e-04 - val_loss: 8.7636e-04\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3540e-04 - val_loss: 8.7560e-04\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3472e-04 - val_loss: 8.7485e-04\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 15s - loss: 9.3406e-04 - val_loss: 8.7412e-04\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3342e-04 - val_loss: 8.7340e-04\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3278e-04 - val_loss: 8.7269e-04\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3216e-04 - val_loss: 8.7199e-04\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3154e-04 - val_loss: 8.7130e-04\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3092e-04 - val_loss: 8.7062e-04\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.3030e-04 - val_loss: 8.6994e-04\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2968e-04 - val_loss: 8.6926e-04\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2906e-04 - val_loss: 8.6859e-04\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2843e-04 - val_loss: 8.6791e-04\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2780e-04 - val_loss: 8.6724e-04\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 16s - loss: 9.2715e-04 - val_loss: 8.6656e-04\n"
     ]
    }
   ],
   "source": [
    "deep_results = train_deep_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=1e-4, decay=1e-9, momentum=0.9, nesterov=True, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266772 samples, validate on 66693 samples\n",
      "Epoch 1/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0155 - val_loss: 0.0041\n",
      "Epoch 2/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 3/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 4/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 6/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 7/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 8/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 10/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 11/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 13/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 14/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 16/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 18/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 19/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 20/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 21/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 22/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 23/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 25/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 26/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 28/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 29/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 32/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 33/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 35/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 36/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 37/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 38/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 39/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 40/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 41/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 42/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 43/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 44/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 45/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 46/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 48/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 49/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 50/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 51/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 52/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 54/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 55/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 56/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 58/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 59/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 60/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 61/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 62/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 63/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 64/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 65/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 66/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 67/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 68/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 69/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 70/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 71/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 72/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 73/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 74/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 75/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 76/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 77/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 78/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 79/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 80/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 81/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 83/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 84/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 85/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 86/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 87/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 88/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 89/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 90/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 91/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 92/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 93/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 94/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 95/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 96/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 97/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 98/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 99/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 100/100\n",
      "266772/266772 [==============================] - 11s - loss: 0.0015 - val_loss: 0.0014\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96400511,  0.8872205 ,  0.8110007 , ...,  0.95634049,\n",
       "         0.6861577 ,  0.96675026],\n",
       "       [ 0.92161393,  0.87873733,  0.83197844, ...,  0.91126007,\n",
       "         0.69899333,  1.00022972],\n",
       "       [ 0.82803655,  0.93806481,  0.9290818 , ...,  0.77344495,\n",
       "         0.69129795,  0.92499137],\n",
       "       ..., \n",
       "       [ 0.17240553,  0.19646129,  0.219879  , ...,  0.25193104,\n",
       "         0.35655159,  0.28895956],\n",
       "       [ 0.54303604,  0.31664708,  0.19415942, ...,  0.64015359,\n",
       "         0.38101375,  0.33227432],\n",
       "       [ 0.95092154,  0.52763247,  0.34683344, ...,  0.99865854,\n",
       "         0.51167536,  0.4727087 ]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.predict(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOMCYmVkqHGDMzCwVDjBmZpaK/x87zT06CxgVTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f7f6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(deep_results[0].history['loss'])\n",
    "plt.plot(deep_results[0].history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['AE train', 'AE test', \"DAE train\", \"DAE test\"], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fd357b8>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
