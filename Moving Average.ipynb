{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from math import sqrt\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv1D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix the random seed to reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lag = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_data():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_X_data()\n",
    "X = pd.DataFrame({\"close\": X[\"close\"]})\n",
    "y = pd.Series(talib.SMA(X[\"close\"].values, lag), index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X[\"2012\":]\n",
    "y = y[\"2012\":]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use difference in X as a predicator\n",
    "# X = X.diff()\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeseries_to_supervised(raw_time_series, lag):\n",
    "    p = {}\n",
    "    for i in range(1, lag+1):\n",
    "        p[\"{}\".format(i)] = raw_time_series.shift(i).fillna(0)\n",
    "    p[\"0\"] = raw_time_series\n",
    "    \n",
    "    if type(raw_time_series) is pd.Series:\n",
    "        supervised_data = pd.DataFrame(p)\n",
    "        supervised_data = pd.Panel({\"0\": supervised_data})\n",
    "        supervised_data = supervised_data.swapaxes(0, 1).swapaxes(1, 2)\n",
    "    else:\n",
    "        supervised_data = pd.Panel(p)\n",
    "    return supervised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_shuffling_train_test_split(X, y, test_size=0.2):\n",
    "    i = int((1 - test_size) * X.shape[0]) + 1\n",
    "    X_train, X_test = np.split(X, [i])\n",
    "    y_train, y_test = np.split(y, [i])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_supervised_X(raw_time_series, lag):\n",
    "    supervised_X = timeseries_to_supervised(X, lag)\n",
    "    if not type(raw_time_series) is pd.Series:\n",
    "        swaped_supervised_X = supervised_X.swapaxes(0, 1)\n",
    "    else:\n",
    "        swaped_supervised_X = supervised_X\n",
    "    return swaped_supervised_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supervised_X = create_supervised_X(X, lag=lag)\n",
    "supervised_X = supervised_X.fillna(0)\n",
    "supervised_X_values = supervised_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500310, 21, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-03-01 13:28:00+0800', tz='Asia/Shanghai')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_X.items[350218:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = non_shuffling_train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350000\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[218:]\n",
    "y_train = y_train[218:]\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test[92:]\n",
    "y_test = y_test[92:]\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(X_train, X_test):\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1), copy=True)\n",
    "    scaler = Normalizer()\n",
    "    # scaler = RobustScaler()\n",
    "    scaler = scaler.fit(X_train)\n",
    "    \n",
    "    if type(X_train) is pd.Series:\n",
    "        train_scaled = pd.Series(scaler.transform(X_train), index=X_train.index)\n",
    "        test_scaled = pd.Series(scaler.transform(X_test), index=X_test.index)\n",
    "    else:\n",
    "        train_scaled = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "        test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    return scaler, train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_scale(scaler, X_train_scaled, X_test_scaled):\n",
    "    X_train = scaler.inverse_transform(X_train_scaled)\n",
    "    X_test = scaler.inverse_transform(X_test_scaled)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train_scaled.index, columns=X_train_scaled.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test_scaled.index, columns=X_test_scaled.columns)\n",
    "    return scaler, X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_series_step = lag\n",
    "features = X_train.shape[1]\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler, X_train_scaled, X_test_scaled = scale(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_supervised_scaled = timeseries_to_supervised(X_train_scaled, time_series_step)\n",
    "X_train_supervised_scaled = X_train_supervised_scaled.swapaxes(0, 1)\n",
    "\n",
    "X_test_supervised_scaled = timeseries_to_supervised(X_test_scaled, time_series_step)\n",
    "X_test_supervised_scaled = X_test_supervised_scaled.swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "yscaler = MinMaxScaler()\n",
    "yscaler.fit(y_train)\n",
    "y_train_scaled = yscaler.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_Model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(128, batch_input_shape=(batch_size, time_series_step+1, features), stateful=True, \n",
    "         return_sequences=True, \n",
    "         activation=\"relu\"\n",
    "        ))\n",
    "    model.add(LSTM(32, activation=\"relu\", stateful=True))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FullyConnected_Model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(time_series_step+1,)))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=1e-8, decay=1e-9, momentum=0.9, nesterov=True, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_Model()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=sgd, metrics=[ 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FullyConnected_Model()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[ 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainFullyConnected_network():\n",
    "    num_epochs = 1\n",
    "    for i in range(num_epochs):\n",
    "        model.fit(\n",
    "            X_train_supervised_scaled.values[:, :, 0],\n",
    "            y_train,\n",
    "            epochs=1000,\n",
    "            batch_size=1000,\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            validation_split=0.2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280000 samples, validate on 70000 samples\n",
      "Epoch 1/1000\n",
      "280000/280000 [==============================] - 1s - loss: 2514040850.7429 - mean_squared_error: 2514040850.7429 - val_loss: 1402506106.5143 - val_mean_squared_error: 1402506106.5143\n",
      "Epoch 2/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2514008633.1429 - mean_squared_error: 2514008633.1429 - val_loss: 1402481287.3143 - val_mean_squared_error: 1402481287.3143\n",
      "Epoch 3/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513975216.9143 - mean_squared_error: 2513975216.9143 - val_loss: 1402456148.1143 - val_mean_squared_error: 1402456148.1143\n",
      "Epoch 4/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513941249.8286 - mean_squared_error: 2513941249.8286 - val_loss: 1402430436.5714 - val_mean_squared_error: 1402430436.5714\n",
      "Epoch 5/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513906559.0857 - mean_squared_error: 2513906559.0857 - val_loss: 1402404143.5429 - val_mean_squared_error: 1402404143.5429\n",
      "Epoch 6/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513871090.7429 - mean_squared_error: 2513871090.7429 - val_loss: 1402377340.3429 - val_mean_squared_error: 1402377340.3429\n",
      "Epoch 7/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513834905.6000 - mean_squared_error: 2513834905.6000 - val_loss: 1402349802.0571 - val_mean_squared_error: 1402349802.0571\n",
      "Epoch 8/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513797896.2286 - mean_squared_error: 2513797896.2286 - val_loss: 1402321444.5714 - val_mean_squared_error: 1402321444.5714\n",
      "Epoch 9/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513760025.6000 - mean_squared_error: 2513760025.6000 - val_loss: 1402292809.1429 - val_mean_squared_error: 1402292809.1429\n",
      "Epoch 10/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513721280.0000 - mean_squared_error: 2513721280.0000 - val_loss: 1402263334.4000 - val_mean_squared_error: 1402263334.4000\n",
      "Epoch 11/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513681683.6571 - mean_squared_error: 2513681683.6571 - val_loss: 1402233541.4857 - val_mean_squared_error: 1402233541.4857\n",
      "Epoch 12/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513641166.1714 - mean_squared_error: 2513641166.1714 - val_loss: 1402202570.9714 - val_mean_squared_error: 1402202570.9714\n",
      "Epoch 13/1000\n",
      "280000/280000 [==============================] - 0s - loss: 2513599721.6000 - mean_squared_error: 2513599721.6000 - val_loss: 1402171320.6857 - val_mean_squared_error: 1402171320.6857\n",
      "Epoch 14/1000\n",
      "183000/280000 [==================>...........] - ETA: 0s - loss: 2788050921.6175 - mean_squared_error: 2788050921.6175"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-133325d50608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainFullyConnected_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-94c4da93375c>\u001b[0m in \u001b[0;36mtrainFullyConnected_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainFullyConnected_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Raw Data to train moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_X_train = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, lag+1):\n",
    "    raw_X_train[\"{}\".format(i)] = X_train.shift(i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "y_std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler.fit(raw_X_train)\n",
    "y_std_scaler.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_X_train_scaled = std_scaler.transform(raw_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "raw_y_train_scaled = y_std_scaler.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_X_train_scaled) == len(raw_y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=1e-7, decay=1e-8, momentum=0.9, nesterov=True)\n",
    "adam = optimizers.adam(lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adadelta = optimizers.Adadelta(lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnected_Model()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=sgd, metrics=[ 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainFullyConnected_network():   \n",
    "    num_epochs = 1\n",
    "    for i in range(num_epochs):\n",
    "        model.fit(\n",
    "            np.nan_to_num(raw_X_train_scaled),\n",
    "            np.nan_to_num(raw_y_train_scaled),\n",
    "            epochs=120,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280000 samples, validate on 70000 samples\n",
      "Epoch 1/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.7463 - mean_squared_error: 0.7463 - val_loss: 1.9695 - val_mean_squared_error: 1.9695\n",
      "Epoch 2/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.5900 - mean_squared_error: 0.5900 - val_loss: 1.5663 - val_mean_squared_error: 1.5663\n",
      "Epoch 3/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.4662 - mean_squared_error: 0.4662 - val_loss: 1.2466 - val_mean_squared_error: 1.2466\n",
      "Epoch 4/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.3684 - mean_squared_error: 0.3684 - val_loss: 0.9931 - val_mean_squared_error: 0.9931\n",
      "Epoch 5/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.2912 - mean_squared_error: 0.2912 - val_loss: 0.7919 - val_mean_squared_error: 0.7919\n",
      "Epoch 6/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.2302 - mean_squared_error: 0.2302 - val_loss: 0.6321 - val_mean_squared_error: 0.6321\n",
      "Epoch 7/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.1821 - mean_squared_error: 0.1821 - val_loss: 0.5053 - val_mean_squared_error: 0.5053\n",
      "Epoch 8/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.1441 - mean_squared_error: 0.1441 - val_loss: 0.4044 - val_mean_squared_error: 0.4044\n",
      "Epoch 9/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.1141 - mean_squared_error: 0.1141 - val_loss: 0.3242 - val_mean_squared_error: 0.3242\n",
      "Epoch 10/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0904 - mean_squared_error: 0.0904 - val_loss: 0.2604 - val_mean_squared_error: 0.2604\n",
      "Epoch 11/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0718 - mean_squared_error: 0.0718 - val_loss: 0.2096 - val_mean_squared_error: 0.2096\n",
      "Epoch 12/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0570 - mean_squared_error: 0.0570 - val_loss: 0.1690 - val_mean_squared_error: 0.1690\n",
      "Epoch 13/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0454 - mean_squared_error: 0.0454 - val_loss: 0.1367 - val_mean_squared_error: 0.1367\n",
      "Epoch 14/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0362 - mean_squared_error: 0.0362 - val_loss: 0.1108 - val_mean_squared_error: 0.1108\n",
      "Epoch 15/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0901 - val_mean_squared_error: 0.0901\n",
      "Epoch 16/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0735 - val_mean_squared_error: 0.0735\n",
      "Epoch 17/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0602 - val_mean_squared_error: 0.0602\n",
      "Epoch 18/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0152 - mean_squared_error: 0.0152 - val_loss: 0.0495 - val_mean_squared_error: 0.0495\n",
      "Epoch 19/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 20/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
      "Epoch 21/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
      "Epoch 22/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0236 - val_mean_squared_error: 0.0236\n",
      "Epoch 23/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
      "Epoch 24/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
      "Epoch 25/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0143 - val_mean_squared_error: 0.0143\n",
      "Epoch 26/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0123 - val_mean_squared_error: 0.0123\n",
      "Epoch 27/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
      "Epoch 28/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "Epoch 29/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 30/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 31/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 32/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 33/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 34/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 35/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 36/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 37/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 38/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 39/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 40/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 41/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 42/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 43/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 44/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 45/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 46/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 47/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 48/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 49/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 50/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 51/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 52/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 53/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 54/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 55/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 56/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 57/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 58/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 59/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 60/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 61/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 62/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 63/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 64/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 65/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 66/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 9.7823e-04 - val_mean_squared_error: 9.7823e-04\n",
      "Epoch 67/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 9.5766e-04 - val_mean_squared_error: 9.5766e-04\n",
      "Epoch 68/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 9.3828e-04 - val_mean_squared_error: 9.3828e-04\n",
      "Epoch 69/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 9.1996e-04 - val_mean_squared_error: 9.1996e-04\n",
      "Epoch 70/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 9.0266e-04 - val_mean_squared_error: 9.0266e-04\n",
      "Epoch 71/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.8631e-04 - val_mean_squared_error: 8.8631e-04\n",
      "Epoch 72/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.7082e-04 - val_mean_squared_error: 8.7082e-04\n",
      "Epoch 73/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.5615e-04 - val_mean_squared_error: 8.5615e-04\n",
      "Epoch 74/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.4219e-04 - val_mean_squared_error: 8.4219e-04\n",
      "Epoch 75/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.2889e-04 - val_mean_squared_error: 8.2889e-04\n",
      "Epoch 76/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.1618e-04 - val_mean_squared_error: 8.1618e-04\n",
      "Epoch 77/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 8.0402e-04 - val_mean_squared_error: 8.0402e-04\n",
      "Epoch 78/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.9238e-04 - val_mean_squared_error: 7.9238e-04\n",
      "Epoch 79/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.8121e-04 - val_mean_squared_error: 7.8121e-04\n",
      "Epoch 80/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.7048e-04 - val_mean_squared_error: 7.7048e-04\n",
      "Epoch 81/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.6014e-04 - val_mean_squared_error: 7.6014e-04\n",
      "Epoch 82/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.5015e-04 - val_mean_squared_error: 7.5015e-04\n",
      "Epoch 83/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.4049e-04 - val_mean_squared_error: 7.4049e-04\n",
      "Epoch 84/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.3114e-04 - val_mean_squared_error: 7.3114e-04\n",
      "Epoch 85/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 7.2209e-04 - val_mean_squared_error: 7.2209e-04\n",
      "Epoch 86/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 7.1330e-04 - val_mean_squared_error: 7.1330e-04\n",
      "Epoch 87/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 7.0473e-04 - val_mean_squared_error: 7.0473e-04\n",
      "Epoch 88/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.9638e-04 - val_mean_squared_error: 6.9638e-04\n",
      "Epoch 89/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.8824e-04 - val_mean_squared_error: 6.8824e-04\n",
      "Epoch 90/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.8031e-04 - val_mean_squared_error: 6.8031e-04\n",
      "Epoch 91/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.7257e-04 - val_mean_squared_error: 6.7257e-04\n",
      "Epoch 92/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.6503e-04 - val_mean_squared_error: 6.6503e-04\n",
      "Epoch 93/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.5766e-04 - val_mean_squared_error: 6.5766e-04\n",
      "Epoch 94/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.5047e-04 - val_mean_squared_error: 6.5047e-04\n",
      "Epoch 95/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.4343e-04 - val_mean_squared_error: 6.4343e-04\n",
      "Epoch 96/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.3653e-04 - val_mean_squared_error: 6.3653e-04\n",
      "Epoch 97/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.2976e-04 - val_mean_squared_error: 6.2976e-04\n",
      "Epoch 98/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.2311e-04 - val_mean_squared_error: 6.2311e-04\n",
      "Epoch 99/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.1657e-04 - val_mean_squared_error: 6.1657e-04\n",
      "Epoch 100/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.1016e-04 - val_mean_squared_error: 6.1016e-04\n",
      "Epoch 101/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 6.0386e-04 - val_mean_squared_error: 6.0386e-04\n",
      "Epoch 102/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.9765e-04 - val_mean_squared_error: 5.9765e-04\n",
      "Epoch 103/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.9153e-04 - val_mean_squared_error: 5.9153e-04\n",
      "Epoch 104/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.8551e-04 - val_mean_squared_error: 5.8551e-04\n",
      "Epoch 105/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.7958e-04 - val_mean_squared_error: 5.7958e-04\n",
      "Epoch 106/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.7373e-04 - val_mean_squared_error: 5.7373e-04\n",
      "Epoch 107/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.6798e-04 - val_mean_squared_error: 5.6798e-04\n",
      "Epoch 108/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.6231e-04 - val_mean_squared_error: 5.6231e-04\n",
      "Epoch 109/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.5672e-04 - val_mean_squared_error: 5.5672e-04\n",
      "Epoch 110/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.5121e-04 - val_mean_squared_error: 5.5121e-04\n",
      "Epoch 111/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.4577e-04 - val_mean_squared_error: 5.4577e-04\n",
      "Epoch 112/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.4040e-04 - val_mean_squared_error: 5.4040e-04\n",
      "Epoch 113/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.3513e-04 - val_mean_squared_error: 5.3513e-04\n",
      "Epoch 114/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.2992e-04 - val_mean_squared_error: 5.2992e-04\n",
      "Epoch 115/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.2479e-04 - val_mean_squared_error: 5.2479e-04\n",
      "Epoch 116/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.1973e-04 - val_mean_squared_error: 5.1973e-04\n",
      "Epoch 117/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.1473e-04 - val_mean_squared_error: 5.1473e-04\n",
      "Epoch 118/120\n",
      "280000/280000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.0980e-04 - val_mean_squared_error: 5.0980e-04\n",
      "Epoch 119/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.0490e-04 - val_mean_squared_error: 5.0490e-04\n",
      "Epoch 120/120\n",
      "280000/280000 [==============================] - 1s - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 5.0006e-04 - val_mean_squared_error: 5.0006e-04\n"
     ]
    }
   ],
   "source": [
    "model = trainFullyConnected_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_X_test = X_test.copy()\n",
    "for i in range(1, lag+1):\n",
    "    raw_X_test[\"{}\".format(i)] = X_test.shift(i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_X_test_scaled = std_scaler.transform(raw_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicated_y_test_scaled = model.predict(raw_X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicated_y = predicated_y.reshape(150000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicated_y = y_std_scaler.inverse_transform(predicated_y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_diff = predicated_y - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.79531025035769"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_diff.abs().sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled = y_std_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(raw_X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "            np.nan_to_num(raw_X_train_scaled),\n",
    "            np.nan_to_num(raw_y_train_scaled),\n",
    "            epochs=20,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            validation_split=0.2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(raw_X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_predicated_scaled = model.predict(raw_X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_predicated = y_std_scaler.inverse_transform(y_test_predicated_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test) == len(y_test_predicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use Raw Data to Train Moving Average LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X_train = X_train.copy()\n",
    "for i in range(1, lag+1):\n",
    "    raw_X_train[\"{}\".format(i)] = X_train.shift(i).fillna(0)\n",
    "\n",
    "X_std_scaler = StandardScaler()\n",
    "y_std_scaler = StandardScaler()\n",
    "\n",
    "X_std_scaler.fit(raw_X_train)\n",
    "y_std_scaler.fit(y_train)\n",
    "\n",
    "raw_X_train_scaled = X_std_scaler.transform(raw_X_train)\n",
    "raw_y_train_scaled = y_std_scaler.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X_test = X_test.copy()\n",
    "for i in range(1, lag+1):\n",
    "    raw_X_test[\"{}\".format(i)] = X_test.shift(i).fillna(0)\n",
    "\n",
    "raw_X_test_scaled = X_std_scaler.transform(raw_X_test)\n",
    "raw_y_test_scaled = y_std_scaler.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X_train_scaled = raw_X_train_scaled.reshape(raw_X_train_scaled.shape[0], time_series_step+1, 1)\n",
    "raw_X_test_scaled = raw_X_test_scaled.reshape(raw_X_test_scaled.shape[0], time_series_step+1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(lstm_layers=None, dense_layers=None):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if lstm_layers:\n",
    "        for i in range(lstm_layers):\n",
    "            model.add(\n",
    "                LSTM(128, batch_input_shape=(batch_size, time_series_step+1, features), stateful=True, \n",
    "                 return_sequences=True, \n",
    "                 activation=\"relu\"))\n",
    "        model.add(LSTM(32, activation=\"relu\", stateful=True))\n",
    "    else:\n",
    "        model.add(\n",
    "            LSTM(128, batch_input_shape=(batch_size, time_series_step+1, features), stateful=True, \n",
    "             return_sequences=True, \n",
    "             activation=\"relu\"\n",
    "            ))\n",
    "        model.add(LSTM(32, activation=\"relu\", stateful=True))\n",
    "        \n",
    "    if dense_layers:\n",
    "        for i in range(dense_layers):\n",
    "            model.add(Dense(128, activation=\"sigmoid\"))\n",
    "        model.add(Dense(1))\n",
    "    else:\n",
    "        model.add(Dense(128))\n",
    "        model.add(Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=1e-8, decay=1e-9, momentum=0.9, nesterov=True, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = optimizers.adam(lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLSTM_network():   \n",
    "    model = LSTM_Model(6, 3)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[ 'mse'])\n",
    "    \n",
    "    num_epochs = 1\n",
    "    for i in range(num_epochs):\n",
    "        model.fit(\n",
    "            np.nan_to_num(raw_X_train_scaled),\n",
    "            np.nan_to_num(raw_y_train_scaled),\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainLSTM_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(raw_X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(raw_X_train_scaled, batch_size=batch_size)\n",
    "y_predicated_scaled = model.predict(raw_X_test_scaled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicatted = y_std_scaler.inverse_transform(y_predicated_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicated_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicated_scaled = model.predict(raw_X_test_scaled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Raw Data to Train Moving Average by ConvNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNet_Model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D()\n",
    "    model.add(Dense(64))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainConv_network():   \n",
    "    num_epochs = 1\n",
    "    for i in range(num_epochs):\n",
    "        model.fit(\n",
    "            np.nan_to_num(raw_X_train_scaled),\n",
    "            np.nan_to_num(raw_y_train_scaled),\n",
    "            epochs=120,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            shuffle=False,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
