{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from pytz import timezone\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "import keras\n",
    "import talib\n",
    "\n",
    "import pywt\n",
    "from statsmodels.robust import stand_mad\n",
    "\n",
    "def get_CU():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X\n",
    "\n",
    "def get_SP500():\n",
    "    import pandas_datareader as pdr    \n",
    "    SP500 = pdr.get_data_yahoo('^GSPC')\n",
    "    return SP500\n",
    "\n",
    "def get_X_data():\n",
    "    import dovahkiin as dk\n",
    "    dp = dk.DataParser()\n",
    "    X = dp.get_data(\"cu\")\n",
    "    return X\n",
    "\n",
    "X = get_X_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_size = (12, 9)\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "X[\"HLC\"] = (X.high + X.low + X.close) / 3\n",
    "X[\"return\"] = X.HLC.pct_change()\n",
    "X[\"return\"] = X[\"return\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_series_step=4\n",
    "\n",
    "def timeseries_to_supervised(raw_time_series, lag):\n",
    "    p = {}\n",
    "    for i in range(1, lag+1):\n",
    "        p[\"{}\".format(i)] = raw_time_series.shift(i).fillna(0)\n",
    "    p[\"0\"] = raw_time_series\n",
    "    \n",
    "    supervised_data = pd.Panel(p)\n",
    "    return supervised_data\n",
    "\n",
    "def non_shuffling_train_test_split(X, y, test_size=0.2):\n",
    "    i = int((1 - test_size) * X.shape[0]) + 1\n",
    "    X_train, X_test = np.split(X, [i])\n",
    "    y_train, y_test = np.split(y, [i])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_supervised_X(raw_time_series, lag):\n",
    "    supervised_X = timeseries_to_supervised(raw_time_series, lag)\n",
    "    swaped_supervised_X = supervised_X.swapaxes(0, 1)\n",
    "    return swaped_supervised_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 958636 elements\n",
      "Total absolute difference: 230.6085907747903\n",
      "Correlation between denoised and original: 0.9060392654130378\n",
      "Test: 33.35%\n",
      "Train: 66.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Wavelet Decomposition\n",
    "level = 9\n",
    "haar = pywt.Wavelet(\"haar\")\n",
    "coeffs = pywt.wavedec(X[\"return\"].values, haar, level=9)\n",
    "recomposed_return = pywt.waverec(coeffs, haar)\n",
    "\n",
    "# Wavelet De-noising\n",
    "sigma = stand_mad(coeffs[-1])\n",
    "uthresh = sigma*np.sqrt(2*np.log(len(X[\"return\"].values)))\n",
    "denoised = coeffs[:]\n",
    "denoised[1:] = (pywt.threshold(i, value=uthresh) for i in denoised[1:])\n",
    "denoised_return = pywt.waverec(denoised, haar)\n",
    "X[\"denoised\"] = denoised_return[:-1]\n",
    "\n",
    "diff = denoised_return[:-1] - X[\"return\"].values\n",
    "correlation = np.corrcoef(denoised_return[:-1], X[\"return\"].values)\n",
    "print(\"There are total of {} elements\".format(len(denoised_return)))\n",
    "print(\"Total absolute difference: {}\".format(np.abs(diff).sum()))\n",
    "print(\"Correlation between denoised and original: {}\".format(correlation[0][1]))\n",
    "\n",
    "#Divide Data into Train and Test\n",
    "\n",
    "X_train = X[\"2012\":\"2015\"]\n",
    "X_test = X[\"2016\":]\n",
    "print(\"Test: {:.2f}%\".format(100 * len(X_test)/len(X[\"2012\":])))\n",
    "print(\"Train: {:.2f}%\".format(100 * len(X_train)/len(X[\"2012\":])))\n",
    "\n",
    "y = X[\"return\"].shift(-1)\n",
    "y = y.fillna(0)\n",
    "y_train = y[\"2012\":\"2015\"]\n",
    "y_test  = y[\"2016\":]\n",
    "\n",
    "del X_train['HLC']\n",
    "del X_train['open']\n",
    "del X_train['high'] \n",
    "del X_train['low']\n",
    "del X_train['close']\n",
    "del X_train['volume']\n",
    "del X_train['openint']\n",
    "\n",
    "del X_test[\"HLC\"]\n",
    "del X_test['open']\n",
    "del X_test['high'] \n",
    "del X_test['low']\n",
    "del X_test['close']\n",
    "del X_test['volume']\n",
    "del X_test['openint']\n",
    "\n",
    "# Normalize de-noised return\n",
    "scaler = StandardScaler()\n",
    "X_train[\"denoised_scaled\"] = scaler.fit_transform(pd.DataFrame(X_train[\"denoised\"].values))\n",
    "X_test[\"denoised_scaled\"] = scaler.transform(pd.DataFrame(X_test[\"denoised\"].values))\n",
    "\n",
    "X_train = create_supervised_X(pd.DataFrame(X_train[\"denoised_scaled\"]), time_series_step)\n",
    "X_test  = create_supervised_X(pd.DataFrame(X_test[\"denoised_scaled\"]), time_series_step)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "batch_size = 15\n",
    "features = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(128, batch_input_shape=(batch_size, time_series_step+1, features), stateful=True, \n",
    "         return_sequences=True, \n",
    "         activation=\"relu\",\n",
    "         kernel_regularizer='l1',\n",
    "          recurrent_regularizer='l1',\n",
    "          bias_regularizer='l1',\n",
    "          dropout=0.1,\n",
    "          recurrent_dropout=0.1\n",
    "        ÷≥≥≥)≥\n",
    ")\n",
    "\n",
    "model.add(\n",
    "    LSTM(128, stateful=True, \n",
    "         return_sequences=True, \n",
    "         activation=\"relu\",\n",
    "         kernel_regularizer='l1',\n",
    "          recurrent_regularizer='l1',\n",
    "          bias_regularizer='l1',\n",
    "          dropout=0.1,\n",
    "          recurrent_dropout=0.1\n",
    "        )\n",
    ")\n",
    "    \n",
    "model.add(\n",
    "    LSTM(32, \n",
    "         activation=\"relu\", \n",
    "         stateful=True, \n",
    "         return_sequences=True,\n",
    "         kernel_regularizer='l1',\n",
    "         recurrent_regularizer='l1',\n",
    "         bias_regularizer='l1',\n",
    "         dropout=0.1,\n",
    "         recurrent_dropout=0.1)\n",
    ")\n",
    "\n",
    "model.add(LSTM(1))\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 333465 samples, validate on 166845 samples\n",
      "Epoch 1/2\n",
      "333465/333465 [==============================] - 915s - loss: 3.8337 - val_loss: 4.5103\n",
      "Epoch 2/2\n",
      "209340/333465 [=================>............] - ETA: 297s - loss: 5.0358"
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "model.fit(X_train.values, \n",
    "          y_train, \n",
    "          epochs=2, \n",
    "          batch_size=batch_size, \n",
    "          shuffle=False, \n",
    "          validation_data=(X_test.values, y_test),\n",
    "          callbacks=[history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
